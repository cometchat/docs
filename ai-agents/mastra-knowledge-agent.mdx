---
title: "Build a Knowledge Agent with Mastra"
description: "Create a Mastra agent that answers questions from your docs, test it locally, and connect it to CometChat."
canonical: "https://www.cometchat.com/docs/ai-agents/mastra-knowledge-agent"
---

Turn your product docs into a friendly chat teammate. This guide mirrors the Chef Agent format: you’ll start with a minimal OpenAI-only agent, add a lightweight knowledge tool (RAG‑lite) to read your Markdown docs, then connect the agent to CometChat.

---

import { Card, CardGroup, Callout, Steps, Tabs, Tab, Columns, Accordion, AccordionGroup, Icon } from 'mintlify';

<Callout>
This guide mirrors the **Chef Agent** walkthrough and swaps the domain for **knowledge retrieval**. You’ll build a minimal agent first, then add doc-based retrieval (RAG-lite), and finally connect it to **CometChat**.
</Callout>

## What You’ll Build

<CardGroup>
  <Card title="Mastra Agent" icon="/images/icons/mastra.svg">
    - A small agent powered by OpenAI.
    - Answers questions with short, accurate responses.
  </Card>
  <Card title="RAG-lite Retrieval">
    - Read local Markdown files as your knowledge base.
    - Return answers with a source list.
  </Card>
  <Card title="CometChat Integration" icon="/images/icons/ai-agents.svg">
    - Connect the agent in the dashboard using Agent ID + Deployment URL.
    - Use in 1:1 or group chats.
  </Card>
</CardGroup>

## Prerequisites

- Node.js 18+
- An OpenAI API key stored as `OPENAI_API_KEY` in `.env`
- Mastra CLI (installed by the init script below)
- Optional: CometChat app (connect later)

## Run the Agent

<Steps>
  <li>

**Create a new Mastra app**

```bash
npx create-mastra@latest
```

  - Follow prompts to name your app (e.g., `my-mastra-app`).
  - Choose **Express** as the backend framework.
  - Select **OpenAI** as the AI provider.

```bash
cd my-mastra-app
npm i openai
echo "OPENAI_API_KEY=sk-xxxxx" > .env
```

  </li>
  <li>

**Project layout** (after `npx create-mastra@latest` + manual knowledge additions)

```
my-mastra-app/
  .env                       # add OPENAI_API_KEY here
  package.json
  tsconfig.json
  mastra.config.ts           # global Mastra configuration (env, logging, etc.)
  src/
    mastra.ts                # exports new Mastra({ agents, tools, ... }) instance
    server.ts                # Express bootstrap (chosen during init)
    agents/
      knowledge/
        config.ts            # agent name/system/model config
        index.ts             # express router (generate endpoint)
        retriever.ts         # added later (RAG-lite retrieval)
    tools/                   # (optional) generic tools live here
    workflows/               # (optional) multi-step orchestrations
    kb/                      # your markdown docs (knowledge base)
      getting-started.md
      pricing.md
```

  </li>
</Steps>

## Create the Agent

Instead of a custom Express router, define a Mastra Agent and register it in `mastra.ts`.

Create **`src/agents/knowledge/agent.ts`**:

```ts
import { Agent } from '@mastra/core/agent';
import { openai } from '@ai-sdk/openai';

export const knowledgeAgent = new Agent({
  name: 'knowledge',
  instructions: `You are a precise knowledge agent for our product.\nIf unsure or out-of-scope, reply: "I don't have that in my notes."\nCite doc titles concisely when possible.`,
  model: openai('gpt-4o-mini')
});
```

Add a lightweight retrieval utility (RAG‑lite) as **`src/agents/knowledge/retriever.ts`** (same as earlier, kept below – unchanged) and a helper tool the agent can invoke.

Create **`src/tools/kb-retrieve.ts`**:

```ts
import { createTool } from '@mastra/core/tools';
import { z } from 'zod';
import { retrieve } from '../agents/knowledge/retriever';

export const kbRetrieveTool = createTool({
  id: 'kb-retrieve',
  description: 'Return top matching knowledge base markdown docs.',
  inputSchema: z.object({ query: z.string().min(1) }),
  outputSchema: z.object({
    sources: z.array(z.object({ title: z.string(), content: z.string() }))
  }),
  execute: async ({ context }) => {
    const docs = retrieve(context.query, 3);
    return { sources: docs };
  }
});
```

Update **`src/agents/knowledge/agent.ts`** to include the tool:

```ts
// ...existing imports...
import { kbRetrieveTool } from '../../tools/kb-retrieve';

export const knowledgeAgent = new Agent({
  name: 'knowledge',
  instructions: `You are a precise knowledge agent for our product.\nAlways call kb-retrieve first to gather context before answering.\nIf docs lack the answer say: "I don't have that in my notes."`,
  model: openai('gpt-4o-mini'),
  tools: { 'kb-retrieve': kbRetrieveTool }
});
```

Create **`src/mastra.ts`**:

```ts
import { Mastra } from '@mastra/core/mastra';
import { knowledgeAgent } from './agents/knowledge/agent';

export const mastra = new Mastra({
  agents: { knowledge: knowledgeAgent }
});
```

Create **`src/server.ts`** (Express runner calling the agent via Mastra):

```ts
import express from 'express';
import bodyParser from 'body-parser';
import { mastra } from './mastra';

const app = express();
app.use(bodyParser.json());

app.post('/api/agents/knowledge/generate', async (req, res) => {
  try {
    const { messages = [] } = req.body || {};
    // Call the Mastra agent (tool usage decided automatically by model)
    const result = await mastra.agents.knowledge.respond({ messages });
    res.json(result);
  } catch (e: any) {
    res.status(500).json({ error: e.message });
  }
});

const PORT = process.env.PORT || 4111;
app.listen(PORT, () => console.log(`Mastra API running on :${PORT}`));
```

> During development you can also run `npx mastra dev` which spins up a similar API automatically.

**Run**

```bash
npm run dev   # or: ts-node src/server.ts / node dist/server.js
```

**Test**

```bash
curl -s http://localhost:4111/api/agents/knowledge/generate   -H "Content-Type: application/json"   -d '{"messages":[{"role":"user","content":"What’s included in pricing?"}]}' | jq .
```

You should receive a short, model-only answer. Next, you’ll ground responses in your docs.

## Create the Tools

Place some docs in **`src/kb/*.md`**. Example: **`src/kb/pricing.md`**

```md
## Pricing
- Free: 100 chats/month
- Pro: 10,000 chats/month, priority support, webhooks
- Enterprise: Custom plan with SSO and SLA
```

Create **`src/agents/knowledge/retriever.ts`**:

```ts
import fs from "fs";
import path from "path";

const KB_DIR = path.join(__dirname, "../../kb");

function tokens(s: string) {
  return s.toLowerCase().replace(/[^a-z0-9\s]/g, " ").split(/\s+/).filter(Boolean);
}
function score(query: string, doc: string) {
  const q = new Set(tokens(query));
  const d = tokens(doc);
  let hits = 0;
  for (const t of d) if (q.has(t)) hits++;
  return hits / Math.sqrt(d.length || 1);
}

export function loadDocs() {
  const files = fs.readdirSync(KB_DIR).filter(f => f.endsWith(".md"));
  return files.map(f => ({
    title: f.replace(".md", ""),
    content: fs.readFileSync(path.join(KB_DIR, f), "utf8"),
  }));
}

export function retrieve(query: string, topK = 3) {
  const docs = loadDocs();
  return docs
    .map(d => ({ ...d, _score: score(query, d.content) }))
    .sort((a, b) => b._score - a._score)
    .slice(0, topK)
    .map(({ title, content }) => ({ title, content }));
}
```

Update **`src/tools/kb-retrieve.ts`** or the retriever logic if you need embeddings later; for now the earlier simple tokenizer stays.

If you prefer NOT to rely on the model to choose the retrieval tool, you can orchestrate manually before calling `respond`:

```ts
// Manual orchestration example (optional)
const docs = retrieve(userQuery, 3);
const systemAugment = `Docs:\n${docs.map(d=>`### ${d.title}\n${d.content}`).join('\n\n---\n\n')}`;
const result = await mastra.agents.knowledge.respond({
  messages: [
    { role: 'system', content: systemAugment },
    ...messages
  ]
});
```

Previous inlined router-based retrieval example (kept here for reference) can be removed once you adopt the tool flow.

<Callout>
The retrieval tool keeps the agent stateless and transparent. You can log tool calls and later swap in a vector DB without changing the agent wiring.
</Callout>

```ts
import { Router } from "express";
import OpenAI from "openai";
import { knowledgeAgent as cfg } from "./config";
import { retrieve } from "./retriever";

export function mountKnowledgeAgent() {
  const router = Router();
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

  router.post("/generate", async (req, res) => {
    try {
      const messages = req.body?.messages ?? [];
      const lastUser = [...messages].reverse().find(m => m.role === "user");
      const query = lastUser?.content ?? "";

      const top = retrieve(query, 3);
      const context = top.map(d => `### ${d.title}\n${d.content}`).join("\n\n---\n\n");

      const completion = await openai.chat.completions.create({
        model: cfg.model.name,
        messages: [
          { role: "system", content: cfg.system },
          { role: "system", content: `Use ONLY the docs below. If missing, say "I don't have that in my notes."\n\n${context}` },
          ...messages,
        ],
      });

      res.json({
        reply: completion.choices[0]?.message?.content ?? "",
        sources: top.map(d => d.title),
        meta: { agent: cfg.name, model: cfg.model.name },
      });
    } catch (e: any) {
      res.status(500).json({ error: e.message });
    }
  });

  return { basePath: `/api/agents/${cfg.name}`, router };
}
```

**Test with retrieval**

```bash
curl -s http://localhost:4111/api/agents/knowledge/generate   -H "Content-Type: application/json"   -d '{"messages":[{"role":"user","content":"What does Pro include?"}]}' | jq .
```

Expected: an answer citing the **pricing** doc and a `sources` array like `["pricing"]`.

<Callout>
This retriever is intentionally simple. Swap it later for Pinecone, Postgres + pgvector, LanceDB, or Elasticsearch. Keep the function signature `retrieve(query, topK)` and the API will remain stable.
</Callout>

## Deploy & Production

<Columns cols={2}>
  <Card title="Deploy the agent">
    - Use your preferred host (Render, Fly.io, Railway, Vercel functions, etc.).  
    - Ensure the `/api/agents/knowledge/generate` endpoint is public.  
    - Keep `OPENAI_API_KEY` in server-side env only.
  </Card>
  <Card title="Connect in CometChat">
    - Go to **CometChat Dashboard → AI Agents**.  
    - Create/Select an agent and set **Provider: Mastra**.  
    - Enter **Agent ID** and **Deployment URL** (your public base URL).  
    - Save, then test in a 1:1 or group conversation.
  </Card>
</Columns>

## Integrate

- Show `sources` as inline chips or “References” links.
- In group chats, allow mentions like `@knowledge What’s our Pro limit?`.
- For safety, add a token/secret expected by your API and include it in CometChat’s outgoing hook.

## Troubleshooting

<AccordionGroup>
  <Accordion title="HTTP 404: Agent not found">
    Ensure your server mounts the router at `/api/agents/knowledge`. Recheck `basePath`.
  </Accordion>
  <Accordion title="401 Unauthorized from OpenAI">
    Verify `OPENAI_API_KEY` in `.env` and that your host exposes it to the runtime.
  </Accordion>
  <Accordion title="Empty or irrelevant answers">
    Add more docs to `src/kb/`. For better ranking, move to embeddings + a vector DB.
  </Accordion>
  <Accordion title="CORS errors in browser calls">
    Route calls through your backend or enable CORS for your domain on the server.
  </Accordion>
</AccordionGroup>

## Next Steps

<Tabs>
  <Tab title="Frontend Actions">
    Teach the agent to emit JSON actions like `{type:"openModal", id:"invite"}` and execute via a client-side dispatcher (whitelisted).
  </Tab>
  <Tab title="Vector DB Retrieval">
    Replace `retriever.ts` with embeddings + Pinecone/pgvector for semantic search and larger corpora.
  </Tab>
  <Tab title="Eval & Guardrails">
    Add prompt tests, refusal patterns, and output validators (Zod) for structured responses.
  </Tab>
</Tabs>
